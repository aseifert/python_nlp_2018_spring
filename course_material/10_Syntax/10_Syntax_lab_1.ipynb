{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Syntax — Lab exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeback form on [Judit's lectures and labs](https://goo.gl/forms/O9Spz4T16Mwj0sZy2).\n",
    "\n",
    "\n",
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this lab, we are going to use the [Python Natural Language Toolkit](http://www.nltk.org/) (`nltk`). It has an API that allows you to create, read, and parse with Context-free Grammars (CFG), as well as to convert parse trees to Chomsky Normal Form (CNF) and back and to display or pretty print them.\n",
    "\n",
    "During the first few exercises, we are going to acquint ourselves with nltk using a toy grammar. In the second part, you will be asked to implement the CKY algorithm and test it on a real world treebank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infrastructure\n",
    "\n",
    "For today's exercises, you will need the docker image again. Provided you have already downloaded it last time, you can start it by:\n",
    "\n",
    "- `docker ps -a`: lists all the containers you have created. One of them should be named after your neptun code.\n",
    "- `docker start <your neptun>`\n",
    "- `docker exec -it <your neptun> bash`\n",
    "\n",
    "In order to be able to run today's exercises, you will have to install some system- and Python packages as well.\n",
    "\n",
    "```bash\n",
    "apt-get install python3-tk\n",
    "pip install graphviz\n",
    "```\n",
    "\n",
    "When that's done, update your git repository:\n",
    "\n",
    "```bash\n",
    "cd /nlp/python_nlp_2018_spring/\n",
    "git pull\n",
    "```\n",
    "\n",
    "And start the notebook:\n",
    "```\n",
    "jupyter notebook --port=8088 --ip=0.0.0.0 --no-browser --allow-root\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boilerplate\n",
    "\n",
    "The following code imports the packages we are going to use. It also defines a function that draws the parse trees with the [`Graphviz`](http://www.graphviz.org/) library. `nltk` _can_ display the trees, but it depends on Tcl, which doesn't work on a headless (GUI-less) system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import nltk\n",
    "from nltk import Nonterminal\n",
    "from nltk.parse.generate import generate\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def does_tcl_work():\n",
    "    \"\"\"Checks if Tcl is installed and works (e.g. it won't on a headless server).\"\"\"\n",
    "    tree = nltk.tree.Tree('test', [])\n",
    "    try:\n",
    "        tree._repr_png_()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def draw_tree(tree):\n",
    "    \"\"\"Draws an NLTK parse tree via Graphviz.\"\"\"\n",
    "    def draw_tree_rec(curr_root, graph, last_node):\n",
    "        node_id = str(int(last_node) + 1)\n",
    "        for child in curr_root:\n",
    "            if isinstance(child, nltk.tree.Tree):\n",
    "                graph.node(node_id, child.label(), penwidth='0')\n",
    "                graph.edge(last_node, node_id, color='darkslategray3', style='bold')\n",
    "                node_id = draw_tree_rec(child, graph, node_id)\n",
    "            else:\n",
    "                graph.node(node_id, child, penwidth='0')\n",
    "                graph.edge(last_node, node_id, color='darkslategray3', style='bold')\n",
    "        return str(int(node_id) + 1)\n",
    "    \n",
    "    graph = graphviz.Graph()\n",
    "    graph.graph_attr['ranksep'] = '0.2'\n",
    "    graph.node('0', tree.label(), penwidth='0')\n",
    "    draw_tree_rec(tree, graph, '0')\n",
    "    return graph._repr_svg_()\n",
    "\n",
    "# Use Graphviz to draw the tree if the Tcl backend of nltk doesn't work\n",
    "if not does_tcl_work():\n",
    "    svg_formatter = get_ipython().display_formatter.formatters['image/svg+xml']\n",
    "    svg_formatter.for_type(nltk.tree.Tree, draw_tree)\n",
    "    # Delete the nltk drawing function, just to be sure\n",
    "    if hasattr(Tree, '_repr_png_'):\n",
    "        delattr(Tree, '_repr_png_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "NLTK is not the only NLP library for Python. [spaCy](https://spacy.io/) is \"industrial-strength\" library which, like NLTK, implements various NLP tools for multiple languages. However, it also supports neural network models (on the GPU as well) and it integrates word vectors. A comparison is [availabe here](https://spacy.io/usage/facts-figures). We teach NLTK in this course because it lends itself better to education and experimentation.\n",
    "\n",
    "However, if you are doing serious NLP work, you should also consider spaCy. But not with the RAM constraints we have to deal with in the lab. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get to know nltk\n",
    "\n",
    "In this exercise, we are using the toy grammar from the lecture with a little modification so that it can handle ditransitives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fromstring() returns a CFG instance from a string\n",
    "# Observe the two ways one can specify alternations in the grammar\n",
    "# and how terminal symbols are specified\n",
    "toy_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> Pronoun | ProperNoun | Det Nominal\n",
    "Nominal -> Nominal Noun\n",
    "Nominal -> Noun\n",
    "VP -> Verb | Verb PP | Verb NP | Verb NP PP | Verb NP NP | Verb NP NP PP\n",
    "PP -> Preposition NP\n",
    "Pronoun -> 'he' | 'she' | 'him' | 'her'\n",
    "ProperNoun -> 'John' | 'Mary' | 'Fido'\n",
    "Det -> 'a' | 'an' | 'the'\n",
    "Noun -> 'flower' | 'bone' | 'necklace' | 'dream' | 'hole' | 'café' | 'house' | 'bed'\n",
    "Verb -> 'loves' | 'gives' | 'gave' | 'sleeps' | 'digs' | 'dag' | 'ate'\n",
    "Preposition -> 'in' | 'on' | 'behind'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for some properties:\n",
    "\n",
    "print('Max RHS length:', toy_grammar.max_len())\n",
    "print('The start symbol is', toy_grammar.start())\n",
    "print('Is it in CNF:', toy_grammar.is_chomsky_normal_form())\n",
    "print('Is this a lexical grammar:', toy_grammar.is_lexical())\n",
    "print('All productions:', toy_grammar.productions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a few sentences\n",
    "for sentence in generate(toy_grammar, n=10):\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, `generate()` only generates the sentences in order. Also, it can run into problems with recursive grammars. Here is a version that generates random sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import count\n",
    "\n",
    "def generate_sample(grammar, start=None):\n",
    "    \"\"\"Generates a single sentence randomly.\"\"\"\n",
    "    gen = [start or grammar.start()]\n",
    "    curr_p = 0\n",
    "    while curr_p < len(gen):\n",
    "        production = random.choice(grammar.productions(lhs=gen[curr_p]))\n",
    "        if production.is_lexical():\n",
    "            gen[curr_p] = production.rhs()[0]\n",
    "            curr_p += 1\n",
    "        else:\n",
    "            gen = gen[:curr_p] + list(production.rhs()) + gen[curr_p + 1:]\n",
    "    return ' '.join(gen)\n",
    "\n",
    "def generate_random(grammar, start=None, n=None):\n",
    "    \"\"\"Generates sentences randomly.\"\"\"\n",
    "    for i in count(0):\n",
    "        yield generate_sample(grammar, start)\n",
    "        if i == n:\n",
    "            break\n",
    "\n",
    "for sentence in generate_random(toy_grammar, n=10):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences can also be parsed. Note that the `parse()` method accepts an `iterable`, where each element is a terminal symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_parser = nltk.ChartParser(toy_grammar)\n",
    "# the split() part is important\n",
    "for tree in toy_parser.parse('John gave Mary a flower in the café'.split()):\n",
    "    display(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parse returns an iterator of `nltk.tree.Tree` objects. This class has some useful functions, such as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the tree to CNF\n",
    "tree.chomsky_normal_form()\n",
    "display(tree)\n",
    "# Let's convert it back...\n",
    "tree.un_chomsky_normal_form()\n",
    "print('The tree has', len(tree), 'children.')\n",
    "print('The first child is another tree:', tree[0])\n",
    "print('All nonterminals are Trees. They have labels:', tree[1].label())\n",
    "print('Terminals are just strings:', tree[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in `nltk`, one can convert a `Tree` to CNF, but not the whole grammar. Also, for full CNF conversion, one needs to run two functions:\n",
    "- [`collapse_unary`](http://www.nltk.org/api/nltk.html#nltk.tree.Tree.collapse_unary). Usually with `collapsePOS=True`;\n",
    "- [`chomsky_normal_form`](http://www.nltk.org/api/nltk.html#nltk.tree.Tree.chomsky_normal_form).\n",
    "\n",
    "The function [`un_chomsky_normal_form`](http://www.nltk.org/api/nltk.html#nltk.tree.Tree.un_chomsky_normal_form) undoes both transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `nltk` has some strange design choices - another being their reliance on Tcl. If you run this notebook on a machine with Tcl installed, a nifty grammar editing tool will pop up if you run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.app.rdparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simple grammars\n",
    "\n",
    "These exercises will help you acquaint yourself with writing context-free grammars. The exercises are not all natural language-related $-$ showing the universality of the formalism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Palindrome\n",
    "\n",
    "Write a grammar that accepts the following language: $L = \\{\\text{w}c\\text{w}^\\text{R} | \\text{w}\\in\\{a,b\\}^*\\}$ ($\\text{w}^\\text{R} = \\text{w}$ backwards).\n",
    "\n",
    "**Note**: the simplest solution (with a single `S` nonterminal **may not work** because of a bug in `nltk.ChartParser`). Use more nonterminals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "pgr = nltk.CFG.fromstring(\"\"\"\n",
    "\"\"\")\n",
    "\n",
    "pparser = nltk.ChartParser(pgr)\n",
    "print(pgr.productions())\n",
    "\n",
    "# Tests\n",
    "assert pparser.parse_one('aca') is not None\n",
    "assert pparser.parse_one('abc') is None\n",
    "assert pparser.parse_one('aacba') is None\n",
    "\n",
    "for tree in pparser.parse('abacaba'):\n",
    "    display(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Restricted palindrome\n",
    "\n",
    "The same as before, but `w` can at most be 3 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "pgr = nltk.CFG.fromstring(\"\"\"\n",
    "\"\"\")\n",
    "\n",
    "pparser = nltk.ChartParser(pgr)\n",
    "\n",
    "# Tests\n",
    "assert pparser.parse_one('aca') is not None\n",
    "assert pparser.parse_one('bbcbb') is not None\n",
    "assert pparser.parse_one('abacaba') is not None\n",
    "assert pparser.parse_one('aacba') is None\n",
    "assert pparser.parse_one('babacabab') is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Agreement\n",
    "\n",
    "Not exactly linguistic agreement, but related. Find a Context-Free Grammar for the following language:\n",
    "$L = \\{\\text{a}^n\\text{b}^m\\text{c}^k: n + m = k\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "agr = nltk.CFG.fromstring(\"\"\"\n",
    "\"\"\")\n",
    "\n",
    "aparser = nltk.ChartParser(agr)\n",
    "\n",
    "# Tests\n",
    "assert aparser.parse_one('aabccc') is not None\n",
    "assert aparser.parse_one('bc') is not None\n",
    "assert aparser.parse_one('aacc') is not None\n",
    "assert aparser.parse_one('aabc') is None\n",
    "assert aparser.parse_one('abbbcccccc') is None\n",
    "assert aparser.parse_one('bacc') is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Arithmetics\n",
    "\n",
    "#### 3.1 Basics\n",
    "Model the four elementary mathematical operations, namely `+`, `-`, `*` and `/`. Your tasks is to validate mathematical expressions that use them. Specifically:\n",
    "- single-digit numbers are valid expressions\n",
    "- if `expr1` and `expr2` are valid expressions, these are also valid:\n",
    "  - `expr1 + expr2`\n",
    "  - `expr1 - expr2`\n",
    "  - `expr1 * expr2`\n",
    "  - `expr1 / expr2`\n",
    "  - `(expr1)`\n",
    "  \n",
    "Try to solve it with as few nonterminals as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "agr = nltk.CFG.fromstring(\"\"\"\n",
    "\"\"\")\n",
    "aparser = nltk.ChartParser(agr)\n",
    "\n",
    "# Tests\n",
    "for tree in aparser.parse('1 - 2 / ( 3 - 4 )'.split()):\n",
    "    display(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Precedence\n",
    "\n",
    "If you implemented the previous task with a single nonterminal, you will see that the grammar is undeterministic, and some parses do not reflect the precedence of mathematical operators. Fix the grammar so that it does!\n",
    "\n",
    "Hints:\n",
    "- `+` and `-` should be higher up the tree than `*` and `/`\n",
    "- you will need at least 3 nonterminals\n",
    "- allow chaining of the same operator types, e.g. `1 + 2 - 3`. One of the nonterminals in the toy grammar above does something similar\n",
    "- do not worry about unit productions, but don't create a unit recursion cycle (e.g. `A -> B -> C -> A`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "# Tests\n",
    "for tree in aparser.parse('1 - 2 / ( 3 - 4 )'.split()):\n",
    "    display(tree)\n",
    "assert len(list(aparser.parse('1 - 2 + 3 / ( 4 - 5 )'.split()))) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 CNF\n",
    "\n",
    "Parse an expression and convert the resulting tree into CNF. If you succeed, congratulations, you can skip this exercise.\n",
    "\n",
    "However, most likely the function will throw an exception. This is because the NLTK algorithm cannot cope with rules that mix nonterminals and terminals in certain ways (e.g. `A -> B '+' C`). Fix your grammar by introducing a POS-like nonterminal (e.g. `add` for `+`) into each such rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "# Tests\n",
    "tree = list(aparser.parse('1 - 2 / ( 3 - 4 )'.split()))[0]\n",
    "tree.chomsky_normal_form()\n",
    "display(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Evaluation*\n",
    "\n",
    "Compute the value of the expression. Implement a recursive function that traverses the tree and returns an interger.\n",
    "\n",
    "Note: if you implemented this function well, but get an `AssertionError` from the last line, it means that your grammar is probably right associative. Look at the (non-CNF) tree to confirm this. If so, make it left associative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tree(tree):\n",
    "    \"\"\"Returns the value of the expression represented by tree.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Tests\n",
    "assert evaluate_tree(next(aparser.parse('1+2'))) == 3\n",
    "assert evaluate_tree(next(aparser.parse('1+2*3'))) == 7\n",
    "assert evaluate_tree(next(aparser.parse('3/(2-3)-4/2-5'))) == -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Treebanks\n",
    "\n",
    "NLTK also contains corpora. Amongst others, it contains about 10% of the Penn TreeBank (PTB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Download\n",
    "\n",
    "Download the corpus with the `nltk.download()` tool. It is under Corpora and is called `treebank`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Corpus statistics\n",
    "\n",
    "The functions below can be used to get the file ids, words, sentences, parse trees from the treebank.\n",
    "\n",
    "Using them, get the following following corpus statistics:\n",
    "- the number of sentences\n",
    "- number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "# PTB file ids\n",
    "print('Ids:', treebank.fileids())\n",
    "\n",
    "# Words in one of the files\n",
    "print('Words:', treebank.words('wsj_0003.mrg'))\n",
    "\n",
    "# Word - POS-tag pairs\n",
    "print('Tagged words:', treebank.tagged_words('wsj_0003.mrg'))\n",
    "\n",
    "display(treebank.parsed_sents('wsj_0003.mrg')[0])\n",
    "\n",
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 POS statistics\n",
    "\n",
    "Using the output of `tagged_words()`, above, create two dictionaries:\n",
    "- `pos_words`: POS tag $\\rightarrow$ words\n",
    "- `word_poss`: word $\\rightarrow$ POS tags\n",
    "\n",
    "With the help of these dictionaries, compute the following statistics:\n",
    "- number of word types\n",
    "- number of POS tags\n",
    "- which POS tag has the most word types\n",
    "- which word has the most POS tags assigned to it\n",
    "\n",
    "**Note**:\n",
    "- use all the files\n",
    "- don't forget to lowercase the words\n",
    "- `defaultdict` might be useful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "word_types = ...\n",
    "pos_tags = ...\n",
    "most_frequent_pos = ...\n",
    "word_with_most_pos = ...\n",
    "\n",
    "# Tests\n",
    "assert word_types == 11387\n",
    "assert pos_tags == 46\n",
    "assert most_frequent_pos == 'NN'\n",
    "assert word_with_most_pos == 'a'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
